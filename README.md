# IntraNest AI

A production-ready enterprise conversational AI platform that combines advanced Retrieval-Augmented Generation (RAG) with complete data sovereignty. Deploy your own ChatGPT/Claude-level AI assistant that operates entirely on your infrastructure.

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![LlamaIndex](https://img.shields.io/badge/LlamaIndex-0.9.48-orange.svg)](https://www.llamaindex.ai/)

## ğŸ—ï¸ Architecture Overview

IntraNest implements a sophisticated multi-tier architecture combining modern AI technologies with enterprise-grade infrastructure for secure, scalable conversational AI.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 User Interface                      â”‚
â”‚           LibreChat + React Frontend               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ HTTPS/SSL
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FastAPI Backend                        â”‚
â”‚     â€¢ REST API endpoints                           â”‚
â”‚     â€¢ Async request handling                       â”‚
â”‚     â€¢ Authentication & authorization               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            LlamaIndex RAG Pipeline                  â”‚
â”‚     â€¢ Query processing & transformation            â”‚
â”‚     â€¢ Context assembly & orchestration             â”‚
â”‚     â€¢ Multi-step reasoning                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Weaviate Vector  â”‚    â”‚     OpenAI GPT-4         â”‚
    â”‚    Database       â”‚    â”‚   Language Model         â”‚
    â”‚ â€¢ 1536-dim vectorsâ”‚    â”‚ â€¢ Response generation    â”‚
    â”‚ â€¢ Semantic search â”‚    â”‚ â€¢ Natural conversation   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Key Features

### Advanced RAG Implementation
- **Hybrid Search**: Combines semantic and keyword search for optimal retrieval
- **Context-Aware Processing**: Maintains conversation context across sessions  
- **Smart Document Chunking**: Intelligent text segmentation with overlap
- **Multi-Format Support**: PDF, DOCX, HTML, TXT, JSON, and Markdown
- **Coreference Resolution**: Understands "it," "this," "that" references

### Enterprise Infrastructure
- **Scalable Architecture**: Microservices-based design with async processing
- **Production Deployment**: AWS EC2 with automated health monitoring
- **Security First**: HTTPS/SSL, OAuth 2.0, Microsoft authentication
- **High Availability**: PM2 process management with auto-recovery
- **Data Sovereignty**: Complete control over data processing and storage

### Technical Capabilities
- **Vector Database**: Weaviate with HNSW indexing for fast similarity search
- **Query Optimization**: LlamaIndex with multi-step reasoning and query transformation
- **Memory Management**: Redis-backed session storage with conversation history
- **Document Pipeline**: Async processing with progress tracking and error handling
- **Cross-Platform**: Web application + native desktop apps (Windows/macOS)

## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Backend API** | FastAPI 0.104+ | High-performance async REST API |
| **RAG Orchestration** | LlamaIndex 0.9.48 | Query processing and retrieval |
| **Vector Database** | Weaviate 1.21.0 | Semantic search and embeddings |
| **LLM Integration** | OpenAI GPT-4 | Natural language generation |
| **Embeddings** | text-embedding-3-small | 1536-dimensional document vectors |
| **Frontend** | LibreChat + React | Conversational user interface |
| **Session Store** | Redis 6.x | In-memory session and conversation data |
| **Database** | MongoDB 7.0 | User management and metadata |
| **Web Server** | Nginx | SSL termination and reverse proxy |
| **Process Manager** | PM2 | Service orchestration and monitoring |
| **Desktop Apps** | Tauri | Cross-platform native applications |

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- Node.js 18+
- Docker & Docker Compose
- OpenAI API key

### Installation

```bash
# Clone the repository
git clone https://github.com/josephwmusso/IntraNest.git
cd IntraNest

# Start infrastructure services
docker-compose -f infrastructure/docker/docker-compose.yml up -d

# Setup Python backend
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your OpenAI API key and other settings

# Start the backend service
python main.py
```

### Frontend Setup

```bash
# In a new terminal, setup LibreChat frontend
cd frontend/LibreChat
npm install
cp .env.example .env
# Configure your environment variables
npm start
```

Access the application at `http://localhost:3090`

## ğŸ“ Project Structure

```
IntraNest/
â”œâ”€â”€ backend/                    # FastAPI backend service
â”‚   â”œâ”€â”€ main.py                # Application entry point
â”‚   â”œâ”€â”€ services/              # Core business logic
â”‚   â”‚   â”œâ”€â”€ llamaindex_service.py    # RAG orchestration
â”‚   â”‚   â”œâ”€â”€ conversation_service.py  # Dialog management
â”‚   â”‚   â”œâ”€â”€ document_processor.py    # File processing pipeline
â”‚   â”‚   â””â”€â”€ cache_service.py         # Redis caching layer
â”‚   â”œâ”€â”€ routers/               # API route definitions
â”‚   â”œâ”€â”€ models/                # Pydantic data models
â”‚   â””â”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ frontend/                   # User interface
â”‚   â””â”€â”€ LibreChat/             # React-based chat interface
â”œâ”€â”€ infrastructure/            # Deployment configurations
â”‚   â”œâ”€â”€ docker/                # Container definitions
â”‚   â”œâ”€â”€ aws/                   # Cloud deployment scripts
â”‚   â””â”€â”€ nginx/                 # Web server configuration
â””â”€â”€ desktop/                   # Cross-platform desktop apps
    â””â”€â”€ tauri-app/             # Tauri desktop application
```

## ğŸ§  RAG Pipeline Deep Dive

### Document Processing Pipeline

1. **File Upload & Validation**
   - Multi-format support with type detection
   - Content extraction with fallback methods
   - Text cleaning and normalization

2. **Intelligent Chunking**
   - Sentence-boundary aware segmentation
   - Configurable chunk size with overlap
   - Metadata preservation throughout pipeline

3. **Embedding Generation**
   - OpenAI text-embedding-3-small (1536 dimensions)
   - Batch processing for efficiency
   - Error handling and retry logic

4. **Vector Storage**
   - Weaviate vector database with HNSW indexing
   - User isolation and multi-tenancy
   - Metadata filtering and hybrid search

### Query Processing Flow

1. **Query Analysis**
   - Intent classification and entity extraction
   - Context integration from conversation history
   - Query transformation and enhancement

2. **Retrieval Strategy**
   - Hybrid semantic + keyword search
   - User-scoped document filtering
   - Relevance scoring and ranking

3. **Context Assembly**
   - Retrieved document synthesis
   - Conversation memory integration
   - Source attribution and citations

4. **Response Generation**
   - GPT-4 integration with system prompts
   - Streaming response support
   - Quality validation and error handling

## ğŸ”§ Configuration

### Environment Variables

```bash
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Weaviate Vector Database
WEAVIATE_URL=http://localhost:8080
WEAVIATE_API_KEY=optional_api_key

# Redis Session Store
REDIS_URL=redis://localhost:6379

# MongoDB User Database
MONGODB_URI=mongodb://localhost:27017/intranest

# RAG Configuration
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
MAX_RETRIEVED_DOCS=10
HYBRID_ALPHA=0.75
```

### Advanced Configuration

See `backend/.env.example` for complete configuration options including:
- Authentication settings (OAuth, API keys)
- Performance tuning parameters
- Logging and monitoring configuration
- Security and CORS settings

## ğŸ¢ Production Deployment

### AWS Infrastructure

IntraNest supports professional AWS deployment with:

- **EC2 Instances**: Auto-scaling compute resources
- **Application Load Balancer**: High availability and SSL termination
- **RDS**: Managed MongoDB-compatible database
- **ElastiCache**: Managed Redis for session storage
- **S3**: Document storage and static assets
- **CloudFront**: Global CDN for optimal performance

### Docker Deployment

```bash
# Build and deploy all services
docker-compose -f docker-compose.prod.yml up -d

# Scale services as needed
docker-compose -f docker-compose.prod.yml up -d --scale backend=3
```

## ğŸ”’ Security Features

- **Data Sovereignty**: All processing on your infrastructure
- **SSL/TLS Encryption**: End-to-end encrypted communications
- **OAuth 2.0**: Microsoft, Google, and custom authentication
- **API Security**: Rate limiting, authentication, and authorization
- **User Isolation**: Multi-tenant data separation
- **Audit Logging**: Complete activity and access logging

## ğŸ“Š Performance & Scalability

### Benchmarks
- **Query Response**: <3 seconds typical (RAG + LLM)
- **Document Processing**: 100-500 docs/hour depending on size
- **Concurrent Users**: 50+ simultaneous conversations
- **Vector Search**: <200ms semantic search latency

### Scaling Options
- **Horizontal Scaling**: Multiple backend instances with load balancing
- **Database Sharding**: Weaviate multi-node clusters
- **Caching**: Multi-tier caching with Redis and application-level cache
- **CDN**: Static asset delivery optimization

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details on:

- Development setup and workflow
- Code style and testing standards  
- Pull request process
- Issue reporting guidelines

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

- **Documentation**: [docs.intranestai.com](https://docs.intranestai.com)
- **Issues**: [GitHub Issues](https://github.com/josephwmusso/IntraNest/issues)
- **Discussions**: [GitHub Discussions](https://github.com/josephwmusso/IntraNest/discussions)

---

**IntraNest AI**: Enterprise-grade conversational AI with complete data sovereignty and unlimited scalability.
